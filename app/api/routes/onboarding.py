"""ì˜¨ë³´ë”© ì „ìš© API ë¼ìš°í„°."""

import json
import logging
from typing import List, Optional

from fastapi import APIRouter, HTTPException, Query, UploadFile, File, Form
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from app.services.gemini_client import get_gemini_client
from app.services.gemini_file_search import upload_document_to_store, get_store_documents
from app.core.config import get_settings

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/onboarding", tags=["onboarding"])

settings = get_settings()

# ì˜¨ë³´ë”©ì—ì„œ ì‚¬ìš©í•  RAG ìŠ¤í† ì–´ë“¤
STORE_PRODUCT = settings.gemini_store_common      # ì œí’ˆ ì§€ì‹ (Freshworks, Google ë“±)
STORE_HANDOVER = settings.gemini_store_onboarding  # ì¸ìˆ˜ì¸ê³„/í”„ë¡œì„¸ìŠ¤ ë¬¸ì„œ


# ============================================
# Request/Response Models
# ============================================

class CreateSessionRequest(BaseModel):
    """ì„¸ì…˜ ìƒì„± ìš”ì²­."""
    userName: str


class CreateSessionResponse(BaseModel):
    """ì„¸ì…˜ ìƒì„± ì‘ë‹µ."""
    sessionId: str
    message: str


class SaveProgressRequest(BaseModel):
    """ì§„í–‰ë„ ì €ì¥ ìš”ì²­."""
    sessionId: str
    scenarioId: str
    choiceId: str
    feedbackRating: Optional[int] = None


# ============================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# ============================================

MENTOR_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ìµœìƒìœ„ í…Œí¬ ê¸°ì—…ì˜ ì‹œë‹ˆì–´ ë©˜í†  'ì˜¨ë³´ë”© ë‚˜ì¹¨ë°˜'ì…ë‹ˆë‹¤.
ì‹ ì…ì‚¬ì›ì˜ ì„±ì¥ì„ ë•ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì—­í• ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ íŠ¹ì§•:
- ë”°ëœ»í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤
- ì‹¤ì§ˆì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸
- ìƒì‚°ì„±, ì‹œê°„ ê´€ë¦¬, ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, ë¬¸ì œ í•´ê²°, í˜‘ì—…ì— ëŒ€í•œ ì „ë¬¸ ì§€ì‹
- í•œêµ­ì–´ë¡œ ë‹µë³€

ì‹ ì…ì‚¬ì›ì´ ê²ªì„ ìˆ˜ ìˆëŠ” ì–´ë ¤ì›€ì— ëŒ€í•´ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”."""


def get_feedback_prompt(
    user_name: str,
    scenario_title: str,
    scenario_description: str,
    all_choices: List[str],
    selected_choice: str
) -> str:
    """ì‹œë‚˜ë¦¬ì˜¤ í”¼ë“œë°± ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸."""
    all_choices_text = '\n'.join(f'- {choice}' for choice in all_choices)
    
    return f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ìµœìƒìœ„ í…Œí¬ ê¸°ì—…ì˜ ë…¸ë ¨í•œ ì‹œë‹ˆì–´ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤. ì‹ ì… ì£¼ë‹ˆì–´ ì‚¬ì› {user_name}ë‹˜ì—ê²Œ ë©˜í† ë§ì„ ì œê³µí•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”.

ì‹ ì…ì‚¬ì›ì—ê²Œ ë‹¤ìŒê³¼ ê°™ì€ ì—…ë¬´ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤:
**ì‹œë‚˜ë¦¬ì˜¤ ì œëª©:** {scenario_title}
**ìƒì„¸ ì„¤ëª…:** {scenario_description}

ì„ íƒ ê°€ëŠ¥í•œ í–‰ë™ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì•˜ìŠµë‹ˆë‹¤:
{all_choices_text}

**ì‹ ì…ì‚¬ì›ì˜ ì„ íƒ:** "{selected_choice}"

ì´ ì„ íƒì— ëŒ€í•´ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°±ì„ ì œê³µí•´ ì£¼ì„¸ìš”. **í”¼ë“œë°±ì€ ë°˜ë“œì‹œ ì•„ë˜ì˜ ë§ˆí¬ë‹¤ìš´ ì„œì‹ì„ ì •í™•íˆ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.**

### ğŸ¤· ë‹¹ì‹ ì˜ ì„ íƒì— ëŒ€í•œ ë¶„ì„
({user_name}ë‹˜ì˜ ì„ íƒì„ ë¨¼ì € ì¸ì •í•˜ê³ , í•´ë‹¹ ì„ íƒì´ ì‹¤ì œ ì—…ë¬´ í™˜ê²½ì—ì„œ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì¥ì ê³¼ ë‹¨ì ì„ ê· í˜• ìˆê²Œ ë¶„ì„)

---

### ğŸ’¡ ì¶”ì²œí•˜ëŠ” ì ‘ê·¼ ë°©ì‹
(ì´ ì‹œë‚˜ë¦¬ì˜¤ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ê°€ì¥ íš¨ê³¼ì ì¸ ì—…ë¬´ ì›ì¹™ì´ë‚˜ ì‚¬ê³  ëª¨ë¸ ì„¤ëª…. ê°€ì¥ ì´ìƒì ì¸ í–‰ë™ê³¼ ê·¸ ì´ìœ ë¥¼ ëª…í™•íˆ ì œì‹œ)

---

### ğŸ¤” ë‹¤ë¥¸ ì„ íƒì§€ë“¤ì— ëŒ€í•œ ê³ ì°°
(ì„ íƒë˜ì§€ ì•Šì€ ë‹¤ë¥¸ ì˜µì…˜ë“¤ì´ ì™œ ëœ íš¨ê³¼ì ì¸ì§€ ê°„ëµí•˜ê²Œ ì„¤ëª…)

---

### â­ í•µì‹¬ ì •ë¦¬
> ({user_name}ë‹˜ì´ ì•ìœ¼ë¡œ ìœ ì‚¬í•œ ìƒí™©ì—ì„œ ê¸°ì–µí•˜ê³  ì ìš©í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ì›ì¹™ì´ë‚˜ êµí›ˆì„ blockquote í˜•ì‹ìœ¼ë¡œ ì‘ì„±)

**í”¼ë“œë°± ì‘ì„±ì´ ëë‚˜ë©´, ë°˜ë“œì‹œ ë‹¤ìŒ ì¤„ì— %%%QUESTIONS%%% ë¼ëŠ” êµ¬ë¶„ìë¥¼ ì‚½ì…í•´ì£¼ì„¸ìš”.**

ê·¸ ë‹¤ìŒ ì¤„ë¶€í„°, ì´ ì£¼ì œì— ëŒ€í•´ ë” ê¹Šì´ ìƒê°í•´ë³¼ ìˆ˜ ìˆëŠ” 3ê°œì˜ ì—°ê´€ ì§ˆë¬¸ì„ ê°ê° í•œ ì¤„ì”© ì‘ì„±í•´ì£¼ì„¸ìš”. ì§ˆë¬¸ ì•ì—ëŠ” ë²ˆí˜¸ë‚˜ ê¸€ë¨¸ë¦¬ ê¸°í˜¸ë¥¼ ë¶™ì´ì§€ ë§ˆì„¸ìš”."""


def get_followup_prompt(
    user_name: str,
    scenario_title: str,
    scenario_description: str,
    original_feedback: str,
    question: str
) -> str:
    """í›„ì† ì§ˆë¬¸ ë‹µë³€ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸."""
    return f"""ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ìµœìƒìœ„ í…Œí¬ ê¸°ì—…ì˜ ì‹œë‹ˆì–´ ë©˜í†  'ì˜¨ë³´ë”© ë‚˜ì¹¨ë°˜'ì…ë‹ˆë‹¤.

**ìƒí™©:**
- **ì‹œë‚˜ë¦¬ì˜¤:** {scenario_title} ({scenario_description})
- **ì´ì „ ì¡°ì–¸ ìš”ì•½:** {original_feedback[:500]}...

ì‹ ì…ì‚¬ì› {user_name}ë‹˜ì´ ë‹¤ìŒê³¼ ê°™ì€ ì¶”ê°€ ì§ˆë¬¸ì„ í–ˆìŠµë‹ˆë‹¤:
**ì§ˆë¬¸:** "{question}"

ì´ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³ , ì‹¤ì§ˆì ì´ë©°, ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ, í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."""


# ============================================
# SSE í—¬í¼
# ============================================

def format_sse(event: str, data: dict) -> str:
    """SSE í¬ë§·ìœ¼ë¡œ ë³€í™˜."""
    return f"event: {event}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"


# ============================================
# ì„¸ì…˜ ê´€ë¦¬ (ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ ì €ì¥)
# ============================================

# ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” Redis/Supabase ì‚¬ìš©
_sessions: dict = {}


@router.post("/session", response_model=CreateSessionResponse)
async def create_session(request: CreateSessionRequest):
    """ì˜¨ë³´ë”© ì„¸ì…˜ ìƒì„±."""
    import uuid
    session_id = f"onboarding-{uuid.uuid4().hex[:8]}"
    
    _sessions[session_id] = {
        "userName": request.userName,
        "conversationHistory": [],
        "progress": [],
    }
    
    logger.info(f"Created onboarding session: {session_id} for user: {request.userName}")
    
    return CreateSessionResponse(
        sessionId=session_id,
        message=f"ì•ˆë…•í•˜ì„¸ìš”, {request.userName}ë‹˜! ì˜¨ë³´ë”© ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    )


# ============================================
# ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (AI ë©˜í† )
# ============================================

@router.get("/chat/stream")
async def chat_stream(
    sessionId: str = Query(...),
    query: str = Query(...),
):
    """AI ë©˜í†  ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° (RAG ê²€ìƒ‰ í¬í•¨)."""
    
    session = _sessions.get(sessionId)
    if not session:
        # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ì„ì‹œ ìƒì„±
        session = {"userName": "ì‹ ì…ì‚¬ì›", "conversationHistory": []}
        _sessions[sessionId] = session
    
    user_name = session.get("userName", "ì‹ ì…ì‚¬ì›")
    history = session.get("conversationHistory", [])
    
    # ì‚¬ìš©í•  RAG ìŠ¤í† ì–´ ëª©ë¡
    rag_stores = []
    if STORE_PRODUCT:
        rag_stores.append(STORE_PRODUCT)
    if STORE_HANDOVER:
        rag_stores.append(STORE_HANDOVER)
    
    async def event_generator():
        try:
            client = get_gemini_client()
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
            messages = [
                {"role": "user", "parts": [{"text": MENTOR_SYSTEM_PROMPT}]},
                {"role": "model", "parts": [{"text": f"ë„¤, {user_name}ë‹˜ì˜ AI ë©˜í† ë¡œì„œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."}]},
            ]
            
            # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 4í„´)
            for turn in history[-4:]:
                messages.append({"role": "user", "parts": [{"text": turn.get("user", "")}]})
                messages.append({"role": "model", "parts": [{"text": turn.get("model", "")}]})
            
            # í˜„ì¬ ì§ˆë¬¸
            messages.append({"role": "user", "parts": [{"text": query}]})
            
            # RAG ê²€ìƒ‰ ì„¤ì • (ì—¬ëŸ¬ ìŠ¤í† ì–´ ë™ì‹œ ê²€ìƒ‰)
            from google.genai import types

            # ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ íŒŒì¼ ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€
            tools = None
            if rag_stores:
                tools = [
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=rag_stores
                        )
                    )
                ]

            generation_config = types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(thinking_budget=0),
                tools=tools,
            )

            full_response = ""

            # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
            model_name = client.models[0]
            response = client.client.models.generate_content_stream(
                model=model_name,
                contents=messages,
                config=generation_config,
            )
            
            for chunk in response:
                if chunk.text:
                    full_response += chunk.text
                    yield format_sse("chunk", {"text": chunk.text})
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            history.append({"user": query, "model": full_response})
            session["conversationHistory"] = history[-10:]  # ìµœê·¼ 10í„´ë§Œ ìœ ì§€
            
            yield format_sse("result", {"text": full_response})
            
        except Exception as e:
            logger.error(f"Chat stream error: {e}")
            yield format_sse("error", {"message": str(e)})
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )


# ============================================
# ì‹œë‚˜ë¦¬ì˜¤ í”¼ë“œë°± ìŠ¤íŠ¸ë¦¬ë°
# ============================================

@router.get("/feedback/stream")
async def feedback_stream(
    sessionId: str = Query(...),
    scenarioId: str = Query(...),
    scenarioTitle: str = Query(...),
    scenarioDescription: str = Query(...),
    selectedChoice: str = Query(...),
    userName: str = Query(...),
    allChoices: List[str] = Query(...),
):
    """ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒì— ëŒ€í•œ í”¼ë“œë°± ìŠ¤íŠ¸ë¦¬ë°."""
    
    async def event_generator():
        try:
            client = get_gemini_client()
            
            prompt = get_feedback_prompt(
                user_name=userName,
                scenario_title=scenarioTitle,
                scenario_description=scenarioDescription,
                all_choices=allChoices,
                selected_choice=selectedChoice,
            )
            
            full_response = ""
            feedback_text = ""
            questions_buffer = ""
            separator_found = False
            separator = "%%%QUESTIONS%%%"
            
            async for chunk in client.generate_content_stream(
                contents=prompt,
                config={"thinking_config": {"thinking_budget": 0}}
            ):
                if chunk.text:
                    chunk_text = chunk.text
                    full_response += chunk_text
                    
                    if separator_found:
                        questions_buffer += chunk_text
                    else:
                        if separator in chunk_text:
                            separator_found = True
                            parts = chunk_text.split(separator)
                            feedback_text += parts[0]
                            if len(parts) > 1:
                                questions_buffer += parts[1]
                            yield format_sse("feedback_chunk", {"text": parts[0]})
                        else:
                            feedback_text += chunk_text
                            yield format_sse("feedback_chunk", {"text": chunk_text})
            
            # í›„ì† ì§ˆë¬¸ íŒŒì‹±
            questions = []
            if questions_buffer:
                questions = [q.strip() for q in questions_buffer.strip().split('\n') if q.strip()]
            
            yield format_sse("questions", {"questions": questions})
            yield format_sse("result", {"text": feedback_text, "questions": questions})
            
        except Exception as e:
            logger.error(f"Feedback stream error: {e}")
            yield format_sse("error", {"message": str(e)})
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )


# ============================================
# í›„ì† ì§ˆë¬¸ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
# ============================================

@router.get("/followup/stream")
async def followup_stream(
    sessionId: str = Query(...),
    scenarioId: str = Query(...),
    scenarioTitle: str = Query(...),
    scenarioDescription: str = Query(...),
    originalFeedback: str = Query(...),
    question: str = Query(...),
    userName: str = Query(...),
):
    """í›„ì† ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°."""
    
    async def event_generator():
        try:
            client = get_gemini_client()
            
            prompt = get_followup_prompt(
                user_name=userName,
                scenario_title=scenarioTitle,
                scenario_description=scenarioDescription,
                original_feedback=originalFeedback,
                question=question,
            )
            
            full_response = ""
            
            async for chunk in client.generate_content_stream(
                contents=prompt,
                config={"thinking_config": {"thinking_budget": 0}}
            ):
                if chunk.text:
                    full_response += chunk.text
                    yield format_sse("chunk", {"text": chunk.text})
            
            yield format_sse("result", {"text": full_response})
            
        except Exception as e:
            logger.error(f"Follow-up stream error: {e}")
            yield format_sse("error", {"message": str(e)})
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )


# ============================================
# ì§„í–‰ë„ ê´€ë¦¬
# ============================================

@router.post("/progress")
async def save_progress(request: SaveProgressRequest):
    """ì‹œë‚˜ë¦¬ì˜¤ ì™„ë£Œ ì§„í–‰ë„ ì €ì¥."""
    from datetime import datetime
    
    session = _sessions.get(request.sessionId)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    progress = session.get("progress", [])
    progress.append({
        "scenarioId": request.scenarioId,
        "choiceId": request.choiceId,
        "feedbackRating": request.feedbackRating,
        "completedAt": datetime.utcnow().isoformat(),
    })
    session["progress"] = progress
    
    logger.info(f"Saved progress for session {request.sessionId}: scenario {request.scenarioId}")
    
    return {"success": True}


@router.get("/progress/{sessionId}")
async def get_progress(sessionId: str):
    """ì§„í–‰ë„ ì¡°íšŒ."""
    session = _sessions.get(sessionId)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    progress = session.get("progress", [])
    
    return {
        "userId": sessionId,
        "userName": session.get("userName", ""),
        "completedScenarios": progress,
        "totalScenarios": 12,  # í•˜ë“œì½”ë”©ëœ ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜
        "completionRate": len(progress) / 12 * 100,
    }


# ============================================
# ë¬¸ì„œ ì—…ë¡œë“œ (ì¸ìˆ˜ì¸ê³„/í”„ë¡œì„¸ìŠ¤ ë¬¸ì„œ)
# ============================================

@router.post("/documents")
async def upload_onboarding_document(
    file: UploadFile = File(...),
    metadata: Optional[str] = Form(None),
):
    """ì˜¨ë³´ë”©/ì¸ìˆ˜ì¸ê³„ ë¬¸ì„œ ì—…ë¡œë“œ."""
    if not STORE_HANDOVER:
        raise HTTPException(
            status_code=500,
            detail="Onboarding store not configured"
        )
    
    try:
        parsed_metadata = []
        if metadata:
            parsed_metadata = json.loads(metadata)
        
        file_content = await file.read()
        result = await upload_document_to_store(
            store_name=STORE_HANDOVER,
            file_name=file.filename or "document.txt",
            file_content=file_content,
            metadata=parsed_metadata,
        )
        
        logger.info(f"Uploaded document: {result.get('displayName')}")
        return result
        
    except Exception as e:
        logger.error(f"Document upload failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/documents")
async def list_onboarding_documents(
    category: Optional[str] = None,
):
    """ì—…ë¡œë“œëœ ì˜¨ë³´ë”© ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ."""
    if not STORE_HANDOVER:
        raise HTTPException(
            status_code=500,
            detail="Onboarding store not configured"
        )
    
    try:
        result = await get_store_documents(STORE_HANDOVER)
        documents = result.get("documents", [])
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
        if category:
            documents = [
                doc for doc in documents
                if any(
                    m.get("key") == "category" and m.get("stringValue") == category
                    for m in (doc.get("customMetadata") or [])
                )
            ]
        
        return documents
        
    except Exception as e:
        logger.error(f"Document list failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/documents/{document_name:path}")
async def delete_onboarding_document(document_name: str):
    """ì˜¨ë³´ë”© ë¬¸ì„œ ì‚­ì œ."""
    from app.services.gemini_file_search import delete_document
    
    try:
        await delete_document(document_name)
        logger.info(f"Deleted document: {document_name}")
        return {"success": True, "deleted": document_name}
        
    except Exception as e:
        logger.error(f"Document delete failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
